{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "eJuris.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1ayrmzX4mESTKRysKeSTsnh0yRp938Scq",
      "authorship_tag": "ABX9TyPMrZY9s/AeF5IOBMrnfSM2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s654R9ru_xkU"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4NV-yBPSuHx"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.data import Dataset\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input\n",
        "from keras.utils import Sequence"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufnj_zHVSpFc"
      },
      "source": [
        "### Loading data in batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z8kx60bqyCN"
      },
      "source": [
        "First, we want build utils functions to load our data in batches so it can fit in memory. Let's start by creating a dictionnary that holds references to labels we want to use when we'll extract numpy compressed files (npz) : those file hold multiple numpy arrays representing each sentence labelled in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9dUVVvAq4dA",
        "outputId": "f3be62b0-04ba-4038-b6cb-dc703dede6df"
      },
      "source": [
        "np_idx = {\"IDs\": [f\"arr_{x}\" for x in range(20089)]}\n",
        "np_idx[\"IDs\"][:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['arr_0',\n",
              " 'arr_1',\n",
              " 'arr_2',\n",
              " 'arr_3',\n",
              " 'arr_4',\n",
              " 'arr_5',\n",
              " 'arr_6',\n",
              " 'arr_7',\n",
              " 'arr_8',\n",
              " 'arr_9']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWIGmQAR_DGo"
      },
      "source": [
        "We want to set apart some of those indices so we can use them as a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0f6tfbA_kDa",
        "outputId": "e4cea456-c0aa-48b0-c8cc-9fd8e2fd1e66"
      },
      "source": [
        "(len(np_idx[\"IDs\"]) * 20) / 100"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4017.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaR9dlyn_H9N"
      },
      "source": [
        "val_ids = []\n",
        "for n in range(4018):\n",
        "    choice = random.choice(np_idx[\"IDs\"])\n",
        "    np_idx[\"IDs\"].remove(choice)\n",
        "    val_ids.append(choice)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPju9y0uAon3",
        "outputId": "ef2b4db7-a4d2-4a78-d28e-d0e0023636d9"
      },
      "source": [
        "len(np_idx[\"IDs\"])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9xOn0MnArYc",
        "outputId": "3329c2ff-0bcd-4cfe-eae8-07e39c3ae9c0"
      },
      "source": [
        "len(val_ids)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4018"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6i6nng9yvv"
      },
      "source": [
        "Let's define some variables to point on our folders and files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJNx8XJN91vI"
      },
      "source": [
        "npz_inputs = \"inputs.npz\"\n",
        "npz_targets = \"targets.npz\"\n",
        "path_to_file = os.path.join(\"drive\", \"MyDrive\", \"Colab Notebooks\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0AwR3P0x_wf"
      },
      "source": [
        "Now we want to build a class that will handle all the unpacking, decompressing and building of dataloaders that we'll use to feed our model progressively.\n",
        "\n",
        "This is strongly inspired from : https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9anAYukQx5Wn"
      },
      "source": [
        "class DataGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, path_to_file, npz_inputs, npz_targets, list_IDs, batch_size=8, dim=(584, 100), n_classes=4, shuffle=True):\n",
        "        self.path_to_file = path_to_file\n",
        "        self.npz_inputs = npz_inputs\n",
        "        self.npz_targets = npz_targets\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"Returns the number of batches per epoch.\"\n",
        "        batches_per_epoch = int(np.floor(len(self.list_IDs)) / self.batch_size)\n",
        "        return batches_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"Generate one batch of data.\"\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        X, Y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"Update indexes after each epoch.\"\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        \"Generate data containing samples of batch_size size.\"\n",
        "\n",
        "        X_temp = []\n",
        "        Y_temp = []\n",
        "\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Loading data\n",
        "            npz_inputs = np.load(os.path.join(self.path_to_file, self.npz_inputs))\n",
        "            X = np.array(npz_inputs[ID])\n",
        "            X_temp.append(X)\n",
        "            # Loading labels\n",
        "            npz_targets = np.load(os.path.join(self.path_to_file, self.npz_targets))\n",
        "            Y = np.array(npz_targets[ID])\n",
        "            Y_temp.append(Y)\n",
        "\n",
        "        return np.asarray(X_temp, dtype=\"float32\"), np.asarray(Y_temp, dtype=\"float32\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGcVxT4y6L6m"
      },
      "source": [
        "train_gen = DataGenerator(path_to_file=path_to_file,\n",
        "                          npz_inputs=npz_inputs,\n",
        "                          npz_targets=npz_targets,\n",
        "                          batch_size=64,\n",
        "                          list_IDs=np_idx[\"IDs\"],\n",
        "                          dim=(584, 100),\n",
        "                          shuffle=True)\n",
        "val_gen = DataGenerator(path_to_file=path_to_file,\n",
        "                        npz_inputs=npz_inputs,\n",
        "                        npz_targets=npz_targets,\n",
        "                        batch_size=64,\n",
        "                        list_IDs=val_ids,\n",
        "                        dim=(584, 100),\n",
        "                        shuffle=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4yVWtu4JiIs",
        "outputId": "4ec908be-8507-46dc-b840-59624a5f42a7"
      },
      "source": [
        "train_gen[4][0].shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 584, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1akPcVqJo2g",
        "outputId": "ddd99e7c-f150-45bc-9f52-f4792b2337f9"
      },
      "source": [
        "train_gen[4][1].shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykMyS9EuPy7m"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpMXKmp7P3Ax"
      },
      "source": [
        "### Simple FNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rRZ1Oq5P4lr"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8V3zijjQA4x"
      },
      "source": [
        "model = Sequential()\n",
        "layers = [Dense(16, activation=\"relu\", input_shape=(584, 100)),\n",
        "          Dense(32, activation=\"relu\"),\n",
        "          Dense(4, activation=\"relu\"),\n",
        "          Flatten(),\n",
        "          Dense(4, activation=\"softmax\")]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHdEk3_UREpc"
      },
      "source": [
        "for layer in layers:\n",
        "    model.add(layer)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk1Tm9kZOrfz",
        "outputId": "580793fa-d22d-4834-9d3c-4d2d45c463f8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 584, 16)           1616      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 584, 32)           544       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 584, 4)            132       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 4)                 9348      \n",
            "=================================================================\n",
            "Total params: 11,640\n",
            "Trainable params: 11,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ulfqrp6RGDB"
      },
      "source": [
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eBovbO9Rabl",
        "outputId": "3953a055-f177-4db3-d7fd-6aeeb5ddd89c"
      },
      "source": [
        "model.fit(x=train_gen,\n",
        "          use_multiprocessing=True,\n",
        "          workers=8,\n",
        "          epochs=1,\n",
        "          validation_data=val_gen,\n",
        "          validation_steps=10,\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "251/251 [==============================] - ETA: 0s - loss: 1.0580 - accuracy: 0.5421 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTjBsWmmKwsI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}